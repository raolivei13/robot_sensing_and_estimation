# Sensing and Estimation in Robotics
This is a repository dedicated to a course that I have taken at the University of California San Diego (ECE276A: Sensing &amp; Estimation in Robotics)

## Project 1

The idea of being able to control and track the position and motion of a robot becomes crucial as the use cases and the environments in which these systems are being used become increasingly more sophisticated. In autonomous vehicles for example, we wish to track the car while it autonomously navigates through the streets of a city, such that it avoids pedestrians, or buildings, or other cars. Other examples such as warehouse robotics, health care robotics, all require some sort of tracking mechanism such that the systems do not produce undesirable outcomes in their respective applications. Therefore the need to control and track the position and motion of these systems involves safety concerns. In this project, we present a method where we are able to track the orientation of a stationary robot undergoing pure rotation, by minimizing an objective function which represents the error between measured sensor data and model predictions. This mathematical formulation will lead us to be able to track the orientation of the robot fairly well when compared to ground truth data.

## Project 2

In the context of autonomy, It is necessary to accurately track and map the motion of a robot’s trajectory and also it’s environment, through the use of sensors, such as LiDAR, cameras, wheel encoders and other types of sensors. The problem of estimating the robot’s position and environment, is a common problem in robotics and it is known as the Simultaneous Localization and Mapping (SLAM) problem. In this project we present a way of approaching the SLAM problem for a Differential-Drive robot via the use of measurements coming from wheel encoders, IMU, LiDAR and a RGBD camera (Kinect). In order to facilitate the Localization part of SLAM, we make use of the Iterative Closest Point (ICP) algorithm to estimate the robot’s pose over time while using LiDAR measurements and also use a library called GTSAM which re-formulates the SLAM problem and optimizes the trajectory of our robot. We then build a occupancy grid of the environment, which addresses the Mapping part of SLAM, using an optimized trajectory obtained from ICP and LiDAR measurements, and GTSAM. Finally, in order to obtain a more accurate view of the robot’s environment, we use the optimized trajectory and also the images captured by the Kinect device to create a texture map of the robot’s environment by projecting these camera images onto the occupancy grid which was created earlier.

## Project 3

The problem of estimating the robot’s position and environment, is a common problem in robotics and it is known as the Simultaneous Localization and Mapping (SLAM) problem. In this project we present a way of solving the SLAM problem by using the Extended Kalman Filter (EKF). Initially, we simply use the Prediction Step of the EKF to track the position of the robot over time (Localization). Then, in order to estimate the location of landmark positions measured by a stereo camera mounted on the robot, we only use the Update Step of the EKF (Mapping). Lastly, the Prediction Step and the Update Step of the EKF are combined such that we perform Localization and Mapping simultaneously, hence the SLAM problem, in this context, takes the name of Visual-Inertial SLAM. We then compare our trajectories with ground truth data and demonstrate that the EKF approach yields fairly good results.
